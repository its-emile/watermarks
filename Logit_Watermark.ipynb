{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/sZzMV21wStiFPeVc+qAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its-emile/watermarks/blob/main/Logit_Watermark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this notebook, I demonstrate how LLM outputs can use a deterministic pattern to choose the likeliest tokens to output next, rather than randomness or the most likely one.\n",
        "\n",
        "Note: GPT2 is obviously awful, with very poorly aligned or incoherent outputs. Any 7B or 13B model would present more coherence. GPT2 is just used here to demonstrate."
      ],
      "metadata": {
        "id": "tbiMw0OR1vUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3bhxj8ioCOT",
        "outputId": "5a72a6c9-ff29-4f50-a2cf-9ab80a3a13da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, AutoModelForCausalLM, AutoTokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating with deterministic logit ranks:\n",
        "Here, I use a deterministic pattern to determine the rank of the next token to output, among the likeliest potential tokens."
      ],
      "metadata": {
        "id": "6Okl5w2B1R5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "watermark = [0,1,1,0,2] # use a unique signature. the model will use this pattern repeatedly as a \"bias\" across its top possible output tokens. You should use low ranks, the program assumes they are no greater than the sequence length.\n",
        "text = \"When you walk your dog, stay safe by\" # Currently based on sentence completion, not an instruct model format (todo)\n",
        "while True:\n",
        "  encoded_input = tokenizer(text, return_tensors='tf')\n",
        "  output = model(encoded_input)\n",
        "  logits = output.logits[0, -1, :]\n",
        "  softmax = tf.math.softmax(logits, axis=-1)\n",
        "  result = tf.math.top_k(softmax, len(watermark))\n",
        "  candidates = \"\"\n",
        "  for r in result:\n",
        "    candidates += \", \"+tokenizer.decode(r)\n",
        "  print(\"candidates:\"+candidates)\n",
        "\n",
        "\n",
        "  # custom rank for the token (NOTE: we could be more greedy to end now if any candidate is a full stop, this would be a TODO option)\n",
        "  rank = watermark[len(encoded_input.input_ids[0]) % len(watermark)]\n",
        "  print(\"selected rank\",rank)\n",
        "\n",
        "  select = result.indices[rank]\n",
        "  token = tokenizer.decode(select)\n",
        "  print(text, \"[\", token, \"]\")\n",
        "  text += token\n",
        "\n",
        "  # stop generating when appropriate for demo.\n",
        "  if ((len(encoded_input.input_ids[0])>50) | (token.find(\".\")>=0)):\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ92MtRou8V7",
        "outputId": "4ac3d262-5672-449e-d803-268d688bfafa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidates:, !!!!!,  your keeping the using following\n",
            "selected rank 2\n",
            "When you walk your dog, stay safe by [  the ]\n",
            "candidates:, !!!!!,  side door leash front dog\n",
            "selected rank 0\n",
            "When you walk your dog, stay safe by the [  side ]\n",
            "candidates:, !!!!!,  of or., and\n",
            "selected rank 1\n",
            "When you walk your dog, stay safe by the side [  or ]\n",
            "candidates:, !!!!!,  the behind in side on\n",
            "selected rank 1\n",
            "When you walk your dog, stay safe by the side or [  behind ]\n",
            "candidates:, !!!!!,  the you a your him\n",
            "selected rank 0\n",
            "When you walk your dog, stay safe by the side or behind [  the ]\n",
            "candidates:, !!!!!,  fence dog door wheel car\n",
            "selected rank 2\n",
            "When you walk your dog, stay safe by the side or behind the [  door ]\n",
            "candidates:, !!!!!, ., and or when\n",
            "selected rank 0\n",
            "When you walk your dog, stay safe by the side or behind the door [ . ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catching on\n",
        "Now, we analyze the \"supplied\" text and show that by recalculating possible tokens one at a time, we can derive the \"rank pattern\" that the message carries."
      ],
      "metadata": {
        "id": "sXc06MuM2Mvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "ranks=[]\n",
        "for i in range(encoded_input.input_ids[0].shape[0]-1):\n",
        "  output = model({'input_ids':encoded_input.input_ids[:,0:i+1],'attention_mask':encoded_input.attention_mask[:,0:i+1]})\n",
        "  logits = output.logits[0, -1, :]\n",
        "  softmax = tf.math.softmax(logits, axis=-1)\n",
        "  result = tf.math.top_k(softmax, len(watermark)).indices.numpy() # we could increase the list here, but we assume the watermark length is the best range to consider.\n",
        "  # what does the model propose as the likeliest next tokens?\n",
        "  candidates = []\n",
        "  for r in result:\n",
        "    candidates.append(r)\n",
        "\n",
        "  # which token is actually next?\n",
        "  test = encoded_input.input_ids[0][i+1].numpy()\n",
        "\n",
        "  # detect rank of the next token, -1 if outside top predictions:\n",
        "  try:\n",
        "    rank = candidates.index(test)\n",
        "  except:\n",
        "    rank = -1\n",
        "  ranks.append(rank)\n",
        "\n",
        "# check on sample tokens' ranks among predictions. This is the \"hidden pattern\" in the text, if any!\n",
        "print(ranks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTCR2QOmv1Ef",
        "outputId": "132c024d-e8ff-4c2e-cdba-04ac1ed81cef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, -1, -1, 0, 0, -1, -1, -1, 2, 0, 1, 1, 0, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the longest run matching the watermark pattern\n",
        "This is still a fairly basic demo, but that length would be indicative of a low-likelihood particular sample unless it was generated and watermarked. Only short runs (or medium with very low ranks) carry some chance of being human.\n",
        "\n",
        "Improvements here will include incorporating levenstein distance to account for any point replacements that might break up very long sequences, but such robustness should present the user with a remark about human generated edits (which may be a *desirable* behavior)."
      ],
      "metadata": {
        "id": "fOn0prGup5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "maxrun=0\n",
        "for k in range(len(ranks)):\n",
        "  run = 0 # start comparing from ranks[k]\n",
        "  # indices of watermark matches\n",
        "  try:\n",
        "    start = watermark.index(ranks[k])\n",
        "    while ranks[k+run] == watermark[start+run % len(watermark)]: # count as long as the pattern matches the watermark\n",
        "      run += 1\n",
        "  except:\n",
        "    if(run>maxrun):\n",
        "      maxrun = run\n",
        "    continue\n",
        "\n",
        "est_likelihood = 1 - 2.72**(-maxrun/2)\n",
        "\n",
        "print(\"detected a sequence as long as\",maxrun,\"tokens matching the watermark pattern (watermark confidence (not original text): \",f\"{int(est_likelihood*100)}/100)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pl_KVcKD6Gn",
        "outputId": "852e66e7-d9d9-4bee-9a9c-18050774c1d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detected a sequence as long as 6 tokens matching the watermark pattern (watermark confidence (not original text):  95/100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since short runs or very low ranks carry some chance of being human, a confidence evaluation would require that expand the loop above to extract the softmax probabilities of these particular tokens. That said, the matched length is a rapid indicator of outputs likely to have been generated with this watermark algorithm."
      ],
      "metadata": {
        "id": "hFe3Im7IT37Q"
      }
    }
  ]
}