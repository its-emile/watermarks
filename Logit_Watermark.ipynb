{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP3TR+uUmnUpJPWzsz7DYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its-emile/watermarks/blob/main/Logit_Watermark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this notebook, I demonstrate how LLM outputs can use a deterministic pattern to choose the likeliest tokens to output next, rather than randomness or the most likely one.\n",
        "\n",
        "Note: GPT2 is obviously awful, with very poorly aligned or incoherent outputs. Any 7B or 13B model would present more coherence."
      ],
      "metadata": {
        "id": "tbiMw0OR1vUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3bhxj8ioCOT",
        "outputId": "7166fae0-9712-4fd6-a353-7e7b303cb7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, AutoModelForCausalLM, AutoTokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating with deterministic logit ranks:\n",
        "Here, I use a deterministic function (token count mod 2) to determine the rank of the next token to output, among the likeliest potential tokens."
      ],
      "metadata": {
        "id": "6Okl5w2B1R5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "watermark = [0,1,1,0,2] # use a unique signature. the model will use this pattern repeatedly as a \"bias\" across its top possible output tokens. To avoid bugs, only use ranks no greater than the sequence length.\n",
        "text = \"When you walk your dog, stay safe by\" # Currently based on sentence completion, not an instruct model format (todo)\n",
        "while True:\n",
        "  encoded_input = tokenizer(text, return_tensors='tf')\n",
        "  output = model(encoded_input)\n",
        "  logits = output.logits[0, -1, :]\n",
        "  softmax = tf.math.softmax(logits, axis=-1)\n",
        "  result = tf.math.top_k(softmax, len(watermark))\n",
        "  candidates = \"\"\n",
        "  for r in result:\n",
        "    candidates += \", \"+tokenizer.decode(r)\n",
        "  print(\"candidates:\"+candidates)\n",
        "\n",
        "\n",
        "  # custom rank for the token:\n",
        "  rank = watermark[len(encoded_input.input_ids[0]) % len(watermark)]\n",
        "  print(\"selected rank\",rank)\n",
        "\n",
        "  select = result.indices[rank]\n",
        "  token = tokenizer.decode(select)\n",
        "  print(text, \"[\", token, \"]\")\n",
        "  text += token\n",
        "\n",
        "  # stop generating when appropriate for demo.\n",
        "  if ((len(encoded_input.input_ids[0])>50) | (token.find(\".\")>=0)):\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ92MtRou8V7",
        "outputId": "6264e95d-7874-4b84-f39b-ad2a68417c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidates:, !!!!!,  your keeping the using following\n",
            "selected rank 2\n",
            "When you walk your dog, stay safe by [  the ]\n",
            "candidates:, !!!!!,  side door leash front dog\n",
            "selected rank 0\n",
            "When you walk your dog, stay safe by the [  side ]\n",
            "candidates:, !!!!!,  of or., and\n",
            "selected rank 1\n",
            "When you walk your dog, stay safe by the side [  or ]\n",
            "candidates:, !!!!!,  the behind in side on\n",
            "selected rank 1\n",
            "When you walk your dog, stay safe by the side or [  behind ]\n",
            "candidates:, !!!!!,  the you a your him\n",
            "selected rank 0\n",
            "When you walk your dog, stay safe by the side or behind [  the ]\n",
            "candidates:, !!!!!,  fence dog door wheel car\n",
            "selected rank 2\n",
            "When you walk your dog, stay safe by the side or behind the [  door ]\n",
            "candidates:, !!!!!, ., and or when\n",
            "selected rank 0\n",
            "When you walk your dog, stay safe by the side or behind the door [ . ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catching on\n",
        "Now, we analyze the \"supplied\" text and show that by recalculating possible tokens one at a time, we can derive the \"rank pattern\" that the message carries."
      ],
      "metadata": {
        "id": "sXc06MuM2Mvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "ranks=[]\n",
        "for i in range(encoded_input.input_ids[0].shape[0]-1):\n",
        "  output = model({'input_ids':encoded_input.input_ids[:,0:i+1],'attention_mask':encoded_input.attention_mask[:,0:i+1]})\n",
        "  logits = output.logits[0, -1, :]\n",
        "  softmax = tf.math.softmax(logits, axis=-1)\n",
        "  result = tf.math.top_k(softmax, len(watermark)).indices.numpy()\n",
        "  # what does the model propose as the likeliest next tokens?\n",
        "  candidates = []\n",
        "  for r in result:\n",
        "    candidates.append(r)\n",
        "\n",
        "  # which token is actually next?\n",
        "  test = encoded_input.input_ids[0][i+1].numpy()\n",
        "\n",
        "  # detect rank of the next token, -1 if outside top predictions:\n",
        "  try:\n",
        "    rank = candidates.index(test)\n",
        "  except:\n",
        "    rank = -1\n",
        "  ranks.append(rank)\n",
        "\n",
        "# check on sample tokens' ranks among predictions\n",
        "print(ranks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTCR2QOmv1Ef",
        "outputId": "7a61d87b-095d-490a-cc1c-a613ff837000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, -1, -1, 0, 0, -1, -1, -1, 2, 0, 1, 1, 0, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline: find the longest run matching the watermark function.\n",
        "# Improvements here will include incorporating levenstein distance to account for point replacements that break up very long sequences.\n",
        "maxrun=0\n",
        "for k in range(len(ranks)):\n",
        "  run = 0 # start comparing from ranks[k]\n",
        "  # indices of watermark matches\n",
        "  try:\n",
        "    start = watermark.index(ranks[k])\n",
        "    while ranks[k+run] == watermark[start+run % len(watermark)]:\n",
        "      run += 1\n",
        "  except:\n",
        "    if(run>maxrun):\n",
        "      maxrun = run\n",
        "    continue\n",
        "\n",
        "print(\"detected a sequence as long as\",maxrun,\"tokens matching the watermark pattern\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pl_KVcKD6Gn",
        "outputId": "6c050136-bfa2-413f-912b-3a5ba44507b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detected a sequence as long as 6 tokens matching the watermark pattern\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not to be fancy but for a confidence evaluation, we may want to expand the loop above to extract the softmax probabilities of these particular tokens. That said, the matched length is a rapid indicator of outputs likely to have been generated with this watermark algorithm."
      ],
      "metadata": {
        "id": "hFe3Im7IT37Q"
      }
    }
  ]
}